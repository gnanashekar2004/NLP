{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9735525,"sourceType":"datasetVersion","datasetId":5958370}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 3\n\n**Name**: Pola Gnana Shekar <br/>\n**Roll No**: 21CS10052","metadata":{}},{"cell_type":"markdown","source":"## FLAN-T5 Model Loading and Inspection\n\nThis section loads both **FLAN-T5 small** and **FLAN-T5 base** models with their tokenizers. FLAN-T5, a variant of T5, is fine-tuned for zero-shot and few-shot NLP tasks.\n\nFor both models, we load and inspect these key features:\n- **Model Architecture**: Includes layers, hidden size, and attention heads.\n- **Vocabulary**: Total tokens the model can recognize.\n- **Parameter Count**: Total model parameters, with base having a larger count than small.\n\nWe also load their **tokenizers** and note:\n- **Vocab Size**: Vocabulary of tokens each model can use.\n- **Padding and EOS Tokens**: IDs for padding sequences and ending inputs consistently.\n\nThe base model, with its additional layers and parameters, is suited for tasks requiring greater complexity, while the small model offers a compact version with similar capabilities.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM,AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:25:52.292848Z","iopub.execute_input":"2024-11-01T04:25:52.293229Z","iopub.status.idle":"2024-11-01T04:25:57.344530Z","shell.execute_reply.started":"2024-11-01T04:25:52.293187Z","shell.execute_reply":"2024-11-01T04:25:57.343691Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(\"Loading Flan-T5 small model....\\n\")\n\nmodel_small = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\ntokenizer_small = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n\nprint(\"Flan-T5 Small Model Features:\")\nprint(f\"Number of Layers: {model_small.config.num_layers}\")\nprint(f\"Hidden Size: {model_small.config.d_model}\")\nprint(f\"Number of Attention Heads: {model_small.config.num_heads}\")\nprint(f\"Vocabulary Size: {model_small.config.vocab_size}\")\nprint(f\"Model Size (parameters): {model_small.num_parameters()}\")\nprint()\n\nprint(\"Tokenizer for Flan-T5 Small:\")\nprint(f\"Tokenizer Vocab Size: {tokenizer_small.vocab_size}\")\nprint(f\"Tokenizer Padding Token ID: {tokenizer_small.pad_token_id}\")\nprint(f\"Tokenizer EOS Token ID: {tokenizer_small.eos_token_id}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T04:25:57.346511Z","iopub.execute_input":"2024-11-01T04:25:57.347252Z","iopub.status.idle":"2024-11-01T04:26:02.321986Z","shell.execute_reply.started":"2024-11-01T04:25:57.347205Z","shell.execute_reply":"2024-11-01T04:26:02.321016Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loading Flan-T5 small model....\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15b200a94f7d48a3b532309be7b9d6b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07ce08401494f459ae943a60279f38d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e020e4f57a34fa0bcd0c8b15b0bbfcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc0a33649ac4e4a9b3c445e88a0a4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f79cd180184fb18b2baab3c040ff05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a145ff0f214aea988452dda72dbea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d3011a072f4e9ca26d4f02c6878a3e"}},"metadata":{}},{"name":"stdout","text":"Flan-T5 Small Model Features:\nNumber of Layers: 8\nHidden Size: 512\nNumber of Attention Heads: 6\nVocabulary Size: 32128\nModel Size (parameters): 76961152\n\nTokenizer for Flan-T5 Small:\nTokenizer Vocab Size: 32100\nTokenizer Padding Token ID: 0\nTokenizer EOS Token ID: 1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Loading Flan-T5 base model....\\n\")\n\nmodel_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\ntokenizer_base = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\nprint(\"Flan-T5 Base Model Features:\")\nprint(f\"Number of Layers: {model_base.config.num_layers}\")\nprint(f\"Hidden Size: {model_base.config.d_model}\")\nprint(f\"Number of Attention Heads: {model_base.config.num_heads}\")\nprint(f\"Vocabulary Size: {model_base.config.vocab_size}\")\nprint(f\"Model Size (parameters): {model_base.num_parameters()}\")\nprint()\n\nprint(\"\\nTokenizer for Flan-T5 Base:\")\nprint(f\"Tokenizer Vocab Size: {tokenizer_base.vocab_size}\")\nprint(f\"Tokenizer Padding Token ID: {tokenizer_base.pad_token_id}\")\nprint(f\"Tokenizer EOS Token ID: {tokenizer_base.eos_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:02.323387Z","iopub.execute_input":"2024-11-01T04:26:02.323993Z","iopub.status.idle":"2024-11-01T04:26:08.900240Z","shell.execute_reply.started":"2024-11-01T04:26:02.323943Z","shell.execute_reply":"2024-11-01T04:26:08.899242Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading Flan-T5 base model....\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6238b31c7f455880c05773ce3e680b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271b8b1f8cce4c98a9de12707a3d0055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209fdb69095c4bf592d4ad1399426c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e654ef6863534b83a1cc25914166dac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6448623e4d4bf2aca8f74496e7ab0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5817ec610c4d82b57455896aa6d6e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f02f30369e3046589368bbcce431882e"}},"metadata":{}},{"name":"stdout","text":"Flan-T5 Base Model Features:\nNumber of Layers: 12\nHidden Size: 768\nNumber of Attention Heads: 12\nVocabulary Size: 32128\nModel Size (parameters): 247577856\n\n\nTokenizer for Flan-T5 Base:\nTokenizer Vocab Size: 32100\nTokenizer Padding Token ID: 0\nTokenizer EOS Token ID: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset Loading and Summary\n\nIn this section, we load and inspect the training, validation, and test datasets for NLP classification. Each dataset is loaded from TSV files without headers, and columns are renamed for consistency.\n\n- **Training Dataset**: \n  - Loaded with columns `speech` and `label`.\n  - Sample data preview provided.\n  - **Statistics**:\n    - Total samples: Displays the count of examples.\n    - **Label Distribution**: Shows counts for each label type.\n\n- **Validation and Test Datasets**: \n  - Loaded similarly with column renaming and sample previews.\n  - **Statistics** for each:\n    - Total samples.\n    - Label distribution by class.\n\nThis overview helps verify dataset integrity and label balance before model training.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:08.902280Z","iopub.execute_input":"2024-11-01T04:26:08.902621Z","iopub.status.idle":"2024-11-01T04:26:09.346989Z","shell.execute_reply.started":"2024-11-01T04:26:08.902586Z","shell.execute_reply":"2024-11-01T04:26:09.346084Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the dataset without headers\ntrain_df = pd.read_csv(\"/kaggle/input/nlpdata/NLP_ass_train.tsv\", sep='\\t', header=None)\n\n# Rename the columns\ntrain_df.columns = ['speech', 'label']\n\nprint(\"Sample data in NLP_train:\")\nprint(train_df.head(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:09.348121Z","iopub.execute_input":"2024-11-01T04:26:09.348764Z","iopub.status.idle":"2024-11-01T04:26:09.442005Z","shell.execute_reply.started":"2024-11-01T04:26:09.348726Z","shell.execute_reply":"2024-11-01T04:26:09.441061Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Sample data in NLP_train:\n                                              speech       label\n0  we cannot continue calling ourselves feminists...      normal\n1                      nawt yall niggers ignoring me      normal\n2  <user> i am bit confused coz chinese ppl can n...  hatespeech\n3  and this is why i end up with nigger trainee d...  hatespeech\n4                  nogs jews and dykes how enriching   offensive \n\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_df = pd.read_csv(\"/kaggle/input/nlpdata/NLP_ass_valid.tsv\",sep='\\t', header=None)\n\n# Rename the columns\nvalidation_df.columns = ['speech', 'label']\n\nprint(\"sample data in NLP_valid:\")\nprint(validation_df.head(),\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:09.443229Z","iopub.execute_input":"2024-11-01T04:26:09.443581Z","iopub.status.idle":"2024-11-01T04:26:09.468383Z","shell.execute_reply.started":"2024-11-01T04:26:09.443545Z","shell.execute_reply":"2024-11-01T04:26:09.467521Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"sample data in NLP_valid:\n                                              speech       label\n0  this bitch in whataburger eating a burger with...  hatespeech\n1  tired i can not support abortion from a moral ...  hatespeech\n2  all of my exes were cute but they were hoes i ...   offensive\n3  <user> <user> they visited helped provided foo...      normal\n4  it funny how people offended by the word nigge...      normal \n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/nlpdata/NLP_ass_test.tsv\",sep='\\t',header=None)\n\n# Rename the columns\ntest_df.columns = ['speech', 'label']\n\nprint(\"sample data in NLP_test:\")\nprint(test_df.head(),\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:09.469564Z","iopub.execute_input":"2024-11-01T04:26:09.469936Z","iopub.status.idle":"2024-11-01T04:26:09.494934Z","shell.execute_reply.started":"2024-11-01T04:26:09.469893Z","shell.execute_reply":"2024-11-01T04:26:09.494048Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"sample data in NLP_test:\n                                              speech       label\n0  i dont think im getting my baby them white 9 h...      normal\n1  laura loomer raped me while screaming at me in...  hatespeech\n2  <user> what did the old lady do was she a nazi...      normal\n3  as much as i appreciate a format of open discu...      normal\n4  sex be so good a bitch be slow stroking and cr...   offensive \n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking the label distribution in each dataset split\nprint(\"Training Set Statistics:\")\nprint(f\"Total samples: {len(train_df)}\")\nprint(\"Label distribution:\\n\", train_df['label'].value_counts(), \"\\n\")\n\nprint(\"Validation Set Statistics (TSV):\")\nprint(f\"Total samples: {len(validation_df)}\")\nprint(\"Label distribution:\\n\", validation_df['label'].value_counts(), \"\\n\")\n\nprint(\"Test Set Statistics:\")\nprint(f\"Total samples: {len(test_df)}\")\nprint(\"Label distribution:\\n\", test_df['label'].value_counts(), \"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:09.495851Z","iopub.execute_input":"2024-11-01T04:26:09.496119Z","iopub.status.idle":"2024-11-01T04:26:09.513816Z","shell.execute_reply.started":"2024-11-01T04:26:09.496089Z","shell.execute_reply":"2024-11-01T04:26:09.512837Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training Set Statistics:\nTotal samples: 15383\nLabel distribution:\n label\nnormal        6251\nhatespeech    4748\noffensive     4384\nName: count, dtype: int64 \n\nValidation Set Statistics (TSV):\nTotal samples: 1922\nLabel distribution:\n label\nnormal        781\nhatespeech    593\noffensive     548\nName: count, dtype: int64 \n\nTest Set Statistics:\nTotal samples: 1924\nLabel distribution:\n label\nnormal        782\nhatespeech    594\noffensive     548\nName: count, dtype: int64 \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation Metrics Calculation\n\nThis code defines a function to compute key evaluation metrics, **accuracy** and **Macro-F1 score**, between predicted and actual labels:\n\n- **Accuracy**: Measures the overall percentage of correct predictions.\n- **Macro-F1 Score**: Calculates the F1 score for each label, then averages them, treating each label equally regardless of frequency (ideal for imbalanced datasets).\n\n### Function Overview:\n- **Input**: \n  - `test_df`: DataFrame with actual labels in the `label` column.\n  - `predictions`: List of model predictions.\n- **Output**:\n  - `accuracy`: The fraction of correctly predicted labels.\n  - `macro_f1`: The Macro-F1 score, emphasizing balanced performance across labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\n# Calculate accuracy and Macro-F1 score\ndef calculate_metrics(test_df, predictions):\n    actual_labels = test_df['label'].str.lower().str.replace(\" \", \"\").tolist()\n    accuracy = accuracy_score(actual_labels, predictions)\n    macro_f1 = f1_score(actual_labels, predictions, average='macro')\n\n    return accuracy, macro_f1","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:09.514933Z","iopub.execute_input":"2024-11-01T04:26:09.515622Z","iopub.status.idle":"2024-11-01T04:26:10.058532Z","shell.execute_reply.started":"2024-11-01T04:26:09.515574Z","shell.execute_reply":"2024-11-01T04:26:10.057630Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Zero-Shot Prompting for Hate Speech Classification\n\nThis code implements a **zero-shot prompting approach** using FLAN-T5 models (small and base) to classify text as \"normal,\" \"hatespeech,\" or \"offensive\" without any labeled examples in the prompt.\n\n### Code Overview:\n1. **get_predictions_zs function**: Processes each sentence in the `test_df` DataFrame, applies the zero-shot prompt (`prompt_zs`), and returns model predictions.\n2. **Prompt format**: A prompt is designed to prompt the model to classify text according to the three labels.\n3. **Model Evaluation**: The `calculate_metrics` function computes accuracy and Macro-F1 scores, allowing us to assess each modelâ€™s performance.\n\nThe code displays sample predictions, providing insight into the modelâ€™s zero-shot performance on the test set.","metadata":{}},{"cell_type":"code","source":"def get_predictions_zs(model, tokenizer, test_df, prompt):\n    all_predictions = []\n    \n    for _, row in test_df.iterrows():\n        text = row['speech']\n        \n        input_text = prompt.format(text)\n        inputs = tokenizer(input_text, return_tensors=\"pt\")\n        outputs = model.generate(**inputs,max_new_tokens=1)\n        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower().replace(\" \", \"\")  # Clean up prediction\n        \n        all_predictions.append(pred)\n    \n    return all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:10.061496Z","iopub.execute_input":"2024-11-01T04:26:10.061989Z","iopub.status.idle":"2024-11-01T04:26:10.068865Z","shell.execute_reply.started":"2024-11-01T04:26:10.061954Z","shell.execute_reply":"2024-11-01T04:26:10.067863Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"prompt_zs = \"Please determine if the text below is 'normal,' 'hatespeech' or 'offensive': {}\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:26:10.069927Z","iopub.execute_input":"2024-11-01T04:26:10.070251Z","iopub.status.idle":"2024-11-01T04:26:10.078986Z","shell.execute_reply.started":"2024-11-01T04:26:10.070218Z","shell.execute_reply":"2024-11-01T04:26:10.078135Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Zero-Shot Classification Using FLAN-T5 Small Model\n\nThe **FLAN-T5 small model** performs zero-shot classification on the test set, using a straightforward prompt to predict labels. Accuracy and Macro-F1 scores provide a quantitative evaluation of the small modelâ€™s zero-shot classification ability. This process highlights the model's ability to generalize with minimal guidance on the task.","metadata":{}},{"cell_type":"code","source":"predictions_small_zs = get_predictions_zs(model_small, tokenizer_small, test_df, prompt_zs)\n\nprint(\"Sample Predictions:\")\nfor i in range(min(20, len(predictions_small_zs))):  # Print at most 5 samples\n    print(f\"Text: {test_df['speech'].iloc[i]} | Prediction: {predictions_small_zs[i]} | Actual Label: {test_df['label'].iloc[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:13:08.295818Z","iopub.execute_input":"2024-11-01T05:13:08.296990Z","iopub.status.idle":"2024-11-01T05:14:58.653382Z","shell.execute_reply.started":"2024-11-01T05:13:08.296944Z","shell.execute_reply":"2024-11-01T05:14:58.652342Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Sample Predictions:\nText: i dont think im getting my baby them white 9 he has two white j and nikes not even touched | Prediction: offensive | Actual Label: normal\nText: laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate | Prediction: offensive | Actual Label: hatespeech\nText: <user> what did the old lady do was she a nazi or white supremacist | Prediction: hate | Actual Label: normal\nText: as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on | Prediction: offensive | Actual Label: normal\nText: sex be so good a bitch be slow stroking and crying | Prediction: normal | Actual Label: offensive\nText: <user> <user> <user> with a room full of white folks | Prediction: hate | Actual Label: normal\nText: i do not blame orlando pie rats for loosing like this they are drunk from that black label ðŸ˜‚ ðŸ˜‚ ðŸ˜‚ mtn 8 ss diski wafa wafa | Prediction: offensive | Actual Label: normal\nText: they are going to figure out a way to kike bitcoin | Prediction: offensive | Actual Label: hatespeech\nText: <user> <user> a camel jockey midde east watering station | Prediction: offensive | Actual Label: normal\nText: then hoes stole my choreo and still managed to make it look bad and lose with it lmfao | Prediction: offensive | Actual Label: offensive\nText: the labour party has <number> mps in prison for paedophilia <number> mps and councillors on the sex registry <number> million white kids raped by paki muslims all labour voters covered up by labour councils mps and a pm | Prediction: of | Actual Label: hatespeech\nText: i raped a sheep because it sneezed at a pig | Prediction: offensive | Actual Label: normal\nText: listening to <number> right now and man these callers are morons in hillbilly voice what if we switched aj hinch with bill o brien would the texans do well good lord | Prediction: normal | Actual Label: normal\nText: also in this same vibe any wigger who calls white missionaries who save the lives of non white children in africa india ect a race traitor is an actual h0m0 | Prediction: normal | Actual Label: offensive\nText: do not worry uk you worked hard cannot afford carehome fees we take your house oh look in london another muzzie family in a nice flat good on you where the tex goes its a shocker | Prediction: offensive | Actual Label: hatespeech\nText: cheating is not the wave grow up you dumbass nigga | Prediction: offensive | Actual Label: normal\nText: do u have to be retarded to be a lino | Prediction: of | Actual Label: normal\nText: <user> <user> why will muslim take refugee in india when bangladesh and pakistan r islamic states isnt india supposed to be unsafe for muslims they should leave for their country asap for their safety | Prediction: of | Actual Label: normal\nText: <user> <user> <user> <user> <user> <user> yeah just the backwoods redneck portion of society in oklahoma that does not even know other sports exist | Prediction: hate | Actual Label: normal\nText: i d rather get fisted by a nigger tbh | Prediction: offensive | Actual Label: offensive\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy_small_zs, macro_f1_small_zs = calculate_metrics(test_df, predictions_small_zs)\n\nprint(f\"Accuracy for Flan-T5 Small Model: {accuracy_small_zs:.4f}\")\nprint(f\"Macro-F1 Score for Flan-T5 Small Model: {macro_f1_small_zs:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:14:58.654979Z","iopub.execute_input":"2024-11-01T05:14:58.655301Z","iopub.status.idle":"2024-11-01T05:14:58.682112Z","shell.execute_reply.started":"2024-11-01T05:14:58.655260Z","shell.execute_reply":"2024-11-01T05:14:58.681236Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy for Flan-T5 Small Model: 0.2448\nMacro-F1 Score for Flan-T5 Small Model: 0.0826\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Zero-Shot Classification Using FLAN-T5 Base Model\n\nThe **FLAN-T5 base model** undergoes a similar zero-shot evaluation on the test set, with the same prompt setup. Due to its larger size and greater representational power, the base model may exhibit higher accuracy and F1 scores. This comparison with the small model enables analysis of performance gains from scaling the model.","metadata":{}},{"cell_type":"code","source":"predictions_base_zs = get_predictions_zs(model_base, tokenizer_base, test_df, prompt_zs)\n\nprint(\"Sample Predictions:\")\nfor i in range(min(20, len(predictions_base_zs))):  # Print at most 5 samples\n    print(f\"Text: {test_df['speech'].iloc[i]} | Prediction: {predictions_base_zs[i]} | Actual Label: {test_df['label'].iloc[i]}\")","metadata":{"execution":{"iopub.status.idle":"2024-11-01T04:33:03.180001Z","shell.execute_reply.started":"2024-11-01T04:28:03.092899Z","shell.execute_reply":"2024-11-01T04:33:03.178904Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Sample Predictions:\nText: i dont think im getting my baby them white 9 he has two white j and nikes not even touched | Prediction: normal | Actual Label: normal\nText: laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate | Prediction: hate | Actual Label: hatespeech\nText: <user> what did the old lady do was she a nazi or white supremacist | Prediction: normal | Actual Label: normal\nText: as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on | Prediction: normal | Actual Label: normal\nText: sex be so good a bitch be slow stroking and crying | Prediction: offensive | Actual Label: offensive\nText: <user> <user> <user> with a room full of white folks | Prediction: normal | Actual Label: normal\nText: i do not blame orlando pie rats for loosing like this they are drunk from that black label ðŸ˜‚ ðŸ˜‚ ðŸ˜‚ mtn 8 ss diski wafa wafa | Prediction: normal | Actual Label: normal\nText: they are going to figure out a way to kike bitcoin | Prediction: normal | Actual Label: hatespeech\nText: <user> <user> a camel jockey midde east watering station | Prediction: normal | Actual Label: normal\nText: then hoes stole my choreo and still managed to make it look bad and lose with it lmfao | Prediction: offensive | Actual Label: offensive\nText: the labour party has <number> mps in prison for paedophilia <number> mps and councillors on the sex registry <number> million white kids raped by paki muslims all labour voters covered up by labour councils mps and a pm | Prediction: normal | Actual Label: hatespeech\nText: i raped a sheep because it sneezed at a pig | Prediction: offensive | Actual Label: normal\nText: listening to <number> right now and man these callers are morons in hillbilly voice what if we switched aj hinch with bill o brien would the texans do well good lord | Prediction: hate | Actual Label: normal\nText: also in this same vibe any wigger who calls white missionaries who save the lives of non white children in africa india ect a race traitor is an actual h0m0 | Prediction: normal | Actual Label: offensive\nText: do not worry uk you worked hard cannot afford carehome fees we take your house oh look in london another muzzie family in a nice flat good on you where the tex goes its a shocker | Prediction: normal | Actual Label: hatespeech\nText: cheating is not the wave grow up you dumbass nigga | Prediction: hate | Actual Label: normal\nText: do u have to be retarded to be a lino | Prediction: normal | Actual Label: normal\nText: <user> <user> why will muslim take refugee in india when bangladesh and pakistan r islamic states isnt india supposed to be unsafe for muslims they should leave for their country asap for their safety | Prediction: normal | Actual Label: normal\nText: <user> <user> <user> <user> <user> <user> yeah just the backwoods redneck portion of society in oklahoma that does not even know other sports exist | Prediction: normal | Actual Label: normal\nText: i d rather get fisted by a nigger tbh | Prediction: hate | Actual Label: offensive\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy_base_zs, macro_f1_base_zs= calculate_metrics(test_df, predictions_base_zs)\n\nprint(f\"Accuracy for Flan-T5 Base Model: {accuracy_base_zs:.4f}\")\nprint(f\"Macro-F1 Score for Flan-T5 Base Model: {macro_f1_base_zs:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:33:03.181694Z","iopub.execute_input":"2024-11-01T04:33:03.182312Z","iopub.status.idle":"2024-11-01T04:33:03.213964Z","shell.execute_reply.started":"2024-11-01T04:33:03.182242Z","shell.execute_reply":"2024-11-01T04:33:03.212911Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy for Flan-T5 Base Model: 0.3643\nMacro-F1 Score for Flan-T5 Base Model: 0.1456\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Few-Shot Prompting for Hate Speech Classification\n\nIn this section, we use a **few-shot prompting** approach, where each model (FLAN-T5 small and base) leverages a small set of labeled examples to improve its classification accuracy.\n\n### Code Overview:\n1. **get_few_shot_examples function**: Selects 3 examples from each class (`normal`, `hatespeech`, and `offensive`) from the `train_df` to create a few-shot prompt.\n2. **create_few_shot_prompt function**: Combines these examples into a full prompt, providing reference samples to guide the modelâ€™s predictions.\n3. **few_shot_inference function**: For each test example, the model uses the few-shot prompt along with the test text for classification.\n4. **Model Evaluation**: We calculate accuracy and Macro-F1 scores to evaluate performance, comparing results for the FLAN-T5 small and base models.\n\nThis approach simulates minimal supervision, allowing the models to benefit from specific in-context examples.","metadata":{}},{"cell_type":"code","source":"import random\n\n# Select 3 examples from each class and shuffle them\ndef get_few_shot_examples(df, examples_per_class=3):\n    few_shot_examples = []\n    for label in df['label'].unique():\n        examples = df[df['label'] == label].sample(examples_per_class, random_state=42)\n        few_shot_examples.extend([(text, label) for text in examples['speech']])\n    # Shuffle all selected examples randomly\n    random.shuffle(few_shot_examples)\n    return few_shot_examples\n\n# Generate the full few-shot prompt with mixed examples\ndef create_few_shot_prompt(few_shot_examples):\n    prompt_text = \"Examples for Hate speech classification task:\\n\"\n    for i, (text, label) in enumerate(few_shot_examples, 1):\n        prompt_text += f\"text: \\\"{text}\\\" â€“ Label: '{label}' |\\n\"\n    return prompt_text\n\n# Testing function: Pass each test example through the few-shot prompt\ndef few_shot_inference(model, tokenizer, test_df, full_prompt):\n    predictions = []\n    for _, row in test_df.iterrows():\n        text = row['speech']\n        # Add the specific test sentence to classify at the end of the full prompt\n        prompt = f\"\\nUsing these references, Please determine if the text below is 'normal,' 'hatespeech' or 'offensive': \\\"{text}\\\"\" + full_prompt \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        outputs = model.generate(**inputs,max_new_tokens=1)\n        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower().replace(\" \", \"\")\n        predictions.append(pred)\n    return predictions\n\nfew_shot_examples = get_few_shot_examples(train_df)\nfull_prompt = create_few_shot_prompt(few_shot_examples)\n\nprint(\"Complete few-shot prompt:\")\nprint(full_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:33:03.215228Z","iopub.execute_input":"2024-11-01T04:33:03.215622Z","iopub.status.idle":"2024-11-01T04:33:03.247307Z","shell.execute_reply.started":"2024-11-01T04:33:03.215585Z","shell.execute_reply":"2024-11-01T04:33:03.246319Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Complete few-shot prompt:\nExamples for Hate speech classification task:\ntext: \"eid mubarak to all my muslim brothers and sisters lt <number>\" â€“ Label: 'normal' |\ntext: \"that nigga kawhi is nice bruh\" â€“ Label: 'normal' |\ntext: \"the ex lying muzrat traitor in chief needs to swing for his crimes\" â€“ Label: 'hatespeech' |\ntext: \"he was like ew really i fucking hate that faggot ðŸ¤£ ðŸ¤£ ðŸ¤£ ðŸ¤£ i lost it you all it was funny as shit\" â€“ Label: 'offensive' |\ntext: \"<user> <user> <user> no we will take care of our beloved indian muslims but we do not want any illegal muslim immigrants from pakistan and bangladesh india is only for indian muslims all hindus sikhs jains and christians\" â€“ Label: 'offensive' |\ntext: \"<user> jesus is gay bt he the good kind of gay not like them fucking gay ponies\" â€“ Label: 'hatespeech' |\ntext: \"refugees welcome but no irish comedies have to be poorly written now so that mexicans and asians will get the jokes yeah you know what life is too short to listen to this radiohead album mpcstatusupdates\" â€“ Label: 'offensive' |\ntext: \"calling me a kike is not an insult\" â€“ Label: 'normal' |\ntext: \"no <number> million white children raped by paki muslims <number> million pakis in the uk for their crime the whole community must be expelled\" â€“ Label: 'hatespeech' |\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Few-Shot Classification Using FLAN-T5 Small Model\n\nThe **FLAN-T5 small model** uses a few-shot prompt for inference, where 3 examples from each label are presented. The prompt aids the model by providing concrete examples, expected to improve classification accuracy. The calculated accuracy and Macro-F1 score assess the impact of these few-shot references on the modelâ€™s performance.","metadata":{}},{"cell_type":"code","source":"few_shot_predictions_small = few_shot_inference(model_small, tokenizer_small, test_df, full_prompt)\n\naccuracy_few_shot, macro_f1_few_shot = calculate_metrics(test_df, few_shot_predictions_small)\n\nprint(f\"Accuracy for Few-Shot Prompting with Flan-T5 Small Model: {accuracy_few_shot:.4f}\")\nprint(f\"Macro-F1 Score for Few-Shot Prompting with Flan-T5 Small Model: {macro_f1_few_shot:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:14:58.684867Z","iopub.execute_input":"2024-11-01T05:14:58.685212Z","iopub.status.idle":"2024-11-01T05:21:26.975281Z","shell.execute_reply.started":"2024-11-01T05:14:58.685179Z","shell.execute_reply":"2024-11-01T05:21:26.974353Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Accuracy for Few-Shot Prompting with Flan-T5 Small Model: 0.3025\nMacro-F1 Score for Few-Shot Prompting with Flan-T5 Small Model: 0.0720\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Few-Shot Classification Using FLAN-T5 Base Model\n\nThe **FLAN-T5 base model** is evaluated with the same few-shot setup, leveraging additional parameters and complexity. By comparing results with the small model, we can observe the base modelâ€™s response to the same few-shot guidance, potentially highlighting improvements in accuracy and F1 scores due to the modelâ€™s larger capacity.","metadata":{}},{"cell_type":"code","source":"few_shot_predictions_base = few_shot_inference(model_base, tokenizer_base, test_df, full_prompt)\n\naccuracy_few_shot, macro_f1_few_shot = calculate_metrics(test_df, few_shot_predictions_base)\n\nprint(f\"Accuracy for Few-Shot Prompting with Flan-T5 Base Model: {accuracy_few_shot:.4f}\")\nprint(f\"Macro-F1 Score for Few-Shot Prompting with Flan-T5 Base Model: {macro_f1_few_shot:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:21:26.977644Z","iopub.execute_input":"2024-11-01T05:21:26.978085Z","iopub.status.idle":"2024-11-01T05:44:25.802829Z","shell.execute_reply.started":"2024-11-01T05:21:26.978038Z","shell.execute_reply":"2024-11-01T05:44:25.801838Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy for Few-Shot Prompting with Flan-T5 Base Model: 0.4064\nMacro-F1 Score for Few-Shot Prompting with Flan-T5 Base Model: 0.1927\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset Overlap Calculation\n\nThis code defines a function `calculate_dataset_overlap` that computes the number of overlapping sentences between three datasets: training, validation, and test sets. \n\n### Key Steps:\n- The function converts the 'speech' columns of the input DataFrames into sets to facilitate intersection operations.\n- It calculates the overlap for the following pairs:\n  - **Training and Validation Sets**: Determines how many sentences are common between these two sets.\n  - **Validation and Test Sets**: Identifies the shared sentences between validation and test datasets.\n  - **Training and Test Sets**: Finds common sentences between the training and test sets.\n\nFinally, the script prints the overlap counts for each of these dataset pairs, providing insight into potential data leakage or redundancy across datasets.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Function to calculate the intersection (overlap) of sentences between different datasets\ndef calculate_dataset_overlap(train_df, val_df, test_df):\n    train_sentences = set(train_df['speech'])\n    val_sentences = set(val_df['speech'])\n    test_sentences = set(test_df['speech'])\n\n    train_val_overlap = len(train_sentences.intersection(val_sentences))\n    val_test_overlap = len(val_sentences.intersection(test_sentences))\n    train_test_overlap = len(train_sentences.intersection(test_sentences))\n\n    return train_val_overlap, val_test_overlap, train_test_overlap\n\ntrain_val_overlap, val_test_overlap, train_test_overlap = calculate_dataset_overlap(train_df, validation_df, test_df)\n\nprint(f\"Number of overlapping sentences between train and validation sets: {train_val_overlap}\")\nprint(f\"Number of overlapping sentences between validation and test sets: {val_test_overlap}\")\nprint(f\"Number of overlapping sentences between train and test sets: {train_test_overlap}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:02:11.489368Z","iopub.execute_input":"2024-11-01T05:02:11.489738Z","iopub.status.idle":"2024-11-01T05:02:11.504249Z","shell.execute_reply.started":"2024-11-01T05:02:11.489704Z","shell.execute_reply":"2024-11-01T05:02:11.503295Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Number of overlapping sentences between train and validation sets: 3\nNumber of overlapping sentences between validation and test sets: 1\nNumber of overlapping sentences between train and test sets: 5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Conclusion\n\nThe performance results for **zero-shot** and **few-shot prompting** with FLAN-T5 on the hate speech classification task reveal distinct patterns across model sizes and prompting strategies:\n\n1. **Zero-Shot Prompting**:\n   - **FLAN-T5 Small Model** achieved an accuracy of 0.2448 and a Macro-F1 score of 0.0826, indicating limited effectiveness for zero-shot hate speech classification.\n   - **FLAN-T5 Base Model** performed better, with an accuracy of 0.3643 and a Macro-F1 score of 0.1456, showing that the base model's larger parameter size may improve generalization in zero-shot settings.\n\n2. **Few-Shot Prompting**:\n   - **FLAN-T5 Small Model** improved slightly in the few-shot setup, with an accuracy of 0.3025 and a Macro-F1 score of 0.0720, although the Macro-F1 score indicates room for improvement in capturing diverse label categories.\n   - **FLAN-T5 Base Model** demonstrated further gains in few-shot prompting, reaching an accuracy of 0.4064 and a Macro-F1 score of 0.1927, suggesting that providing labeled examples in the prompt significantly boosts performance.\n\n### Summary\nOverall, the **FLAN-T5 Base Model** with **few-shot prompting** outperformed other configurations, highlighting the benefit of few-shot examples and increased model capacity for nuanced tasks like hate speech classification. However, further fine-tuning or dataset-specific training would likely yield higher accuracy and Macro-F1 scores.","metadata":{}}]}